{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcsL4ieHTDHE",
        "outputId": "afeb474f-598d-42f6-838f-e9fd1613b200"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.34.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-0.34.0-py3-none-any.whl (135 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/136.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m133.1/136.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"Sua_Key\"\n"
      ],
      "metadata": {
        "id": "ALmr8HYJT-aW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BNnXMkQ7S1_X"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "import os\n",
        "load_dotenv()\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API\"),  # This is the default and can be omitted\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- User: Mensagem do usuário\n",
        "- Assistant: Resposta do modelo\n",
        "- System: Instruções de comportamento para o modelo\n"
      ],
      "metadata": {
        "id": "z9PBTPgJUiKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"Você é um especialista em Inteligência Artificial focado em explicar conceitos \"\n",
        "                \"de forma clara, estruturada e prática. Sempre inicie suas respostas com um \"\n",
        "                \"resumo em até 3 linhas, depois detalhe com exemplos reais e aplicações no mundo \"\n",
        "                \"dos negócios e tecnologia. Evite termos excessivamente técnicos sem explicação \"\n",
        "                \"e, quando necessário, use analogias simples. \"\n",
        "                \"Se o usuário pedir código, forneça exemplos em Python bem comentados.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explique a importância de LLMs com baixa latência.\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CzUQ5L3URmU",
        "outputId": "ebbe1c0f-e07c-48f5-9d72-3d774f69c693"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Resumo (até 3 linhas)**  \n",
            "LLMs (Large Language Models) com baixa latência respondem rapidamente, garantindo experiências fluidas, decisões em tempo real e custos operacionais menores. Essa rapidez é crucial para aplicativos interativos, suporte ao cliente e processos críticos que não podem esperar.  \n",
            "\n",
            "---  \n",
            "\n",
            "## 1. Por que a latência importa?  \n",
            "\n",
            "| Aspecto | Impacto da alta latência | Benefício da baixa latência |\n",
            "|---|---|---|\n",
            "| **Experiência do usuário** | Usuários abandonam a sessão se a resposta demorar > 2 s (estudo da Google). | Interações “conversacionais” se sentem naturais, como conversar com uma pessoa. |\n",
            "| **Decisões em tempo real** | Algoritmos de trading, detecção de fraudes ou manutenção preditiva precisam de respostas em milissegundos; atrasos podem gerar perdas financeiras. | Acelera a tomada de decisão, reduz risco e aumenta a competitividade. |\n",
            "| **Custos de infraestrutura** | Servidores ficam ocupados por mais tempo, aumentando consumo de CPU/GPU e energia. | Menor tempo de uso de recursos reduz contas de cloud e a pegada de carbono. |\n",
            "| **Escalabilidade** | Latência alta limita o número de usuários simultâneos que o sistema suporta. | Mais requisições por segundo (RPS) sem necessidade de multiplicar servidores. |\n",
            "\n",
            "> **Analogia:** pense em um carro de corrida (alta latência) que só acelera depois de 10 s; já um carro de Fórmula 1 (baixa latência) sai da linha de partida quase que instantaneamente. Nos negócios, quem tem “carro de Fórmula 1” chega primeiro ao cliente e ganha a corrida.  \n",
            "\n",
            "---  \n",
            "\n",
            "## 2. Exemplos reais de aplicações onde baixa latência é decisiva  \n",
            "\n",
            "### a) Assistentes virtuais e chatbots de atendimento  \n",
            "- **Caso:** Uma operadora de telecomunicações implementou um chatbot para abrir tickets. Quando a resposta leva mais de 1 s, o cliente sente frustração e abandona a conversa.  \n",
            "- **Resultado com baixa latência:** 30 % de aumento na taxa de resolução no primeiro contato e 15 % de redução nos custos de call‑center.  \n",
            "\n",
            "### b) Ferramentas de codificação assistida (e.g., GitHub Copilot, Tabnine)  \n",
            "- **Caso:** Desenvolvedores precisam de sugestões de código enquanto digitam. Uma latência de 200 ms mantém o fluxo de trabalho; 2 s interrompem a concentração e diminuem a produtividade.  \n",
            "- **Resultado:** Empresas que adotaram LLMs de baixa latência reportam até 20 % de aumento na velocidade de entrega de features.  \n",
            "\n",
            "### c) Sistemas de recomendação em e‑commerce  \n",
            "- **Caso:** Um site de moda recomenda produtos enquanto o usuário rola a página. Cada segundo de espera pode reduzir a taxa de conversão em até 7 %.  \n",
            "- **Resultado:** Redução da latência de 1 s para 150 ms aumentou as vendas em 4,5 % em um trimestre.  \n",
            "\n",
            "### d) FinTechs e trading algorítmico  \n",
            "- **Caso:** Algoritmos de alta frequência analisam notícias e sentimentos de mercado usando LLMs. A diferença entre 100 ms e 500 ms pode significar dezenas de milhares de dólares.  \n",
            "- **Resultado:** Implementação de modelos otimizados para inferência (quantização, compilação) reduziu a latência para 70 ms, gerando um retorno anual de 3,2 M USD.  \n",
            "\n",
            "---  \n",
            "\n",
            "## 3. Como alcançar baixa latência em LLMs  \n",
            "\n",
            "1. **Modelos otimizados** – Use versões “distiladas” ou “quantizadas” (menos bits por peso) que mantêm boa qualidade, mas são menores e rodam mais rápido.  \n",
            "2. **Hardware adequado** – GPUs modernas (NVIDIA H100) ou inferência em **TPU**/CPU com suporte a **AVX‑512** aceleram cálculos matriciais.  \n",
            "3. **Pipeline de inferência** –  \n",
            "   - **Batching inteligente:** agrupar requisições semelhantes em um único lote para aproveitar paralelismo.  \n",
            "   - **Caching de KV‑cache:** em modelos de atenção, guardar resultados de tokens já processados evita recalcular a mesma informação em sequências longas.  \n",
            "4. **Edge deployment** – Colocar o modelo próximo ao usuário (ex.: em servidores de borda ou até dispositivos locais) diminui a latência de rede.  \n",
            "5. **Serviços serverless especializados** – Plataformas como **Amazon SageMaker JumpStart**, **Google Vertex AI** ou **Azure OpenAI** oferecem endpoints otimizados e escaláveis, com SLA de milissegundos.  \n",
            "\n",
            "---  \n",
            "\n",
            "## 4. Impacto nos negócios  \n",
            "\n",
            "| Setor | Métrica que melhora com baixa latência | Exemplo de ganho |\n",
            "|---|---|---|\n",
            "| **Varejo online** | Taxa de conversão, valor médio do pedido | +4 % em vendas em 3 meses |\n",
            "| **Saúde digital** | Tempo de resposta em triagens virtuais | Redução de 30 % em tempo de diagnóstico preliminar |\n",
            "| **Logística** | Otimização de roteamento em tempo real | Economia de 5 % em custos de transporte |\n",
            "| **Educação** | Interatividade em tutores virtuais | Aumento de 15 % no engajamento dos alunos |\n",
            "\n",
            "---  \n",
            "\n",
            "## 5. Checklist rápido para avaliar a latência do seu LLM  \n",
            "\n",
            "1. **Medição:** Use ferramentas como `locust` ou `hey` para medir tempo médio (ms) por request.  \n",
            "2. **Meta:** Defina um SLA (ex.: ≤ 200 ms para chat, ≤ 50 ms para recomendações).  \n",
            "3. **Otimização:**  \n",
            "   - [ ] Modelo quantizado?  \n",
            "   - [ ] KV‑cache habilitado?  \n",
            "   - [ ] Inferência em GPU/TPU?  \n",
            "   - [ ] Endpoint próximo ao usuário?  \n",
            "4. **Teste de carga:** Simule picos de tráfego e verifique se o SLA se mantém.  \n",
            "5. **Monitoramento contínuo:** Alerts quando a latência ultrapassa o limite.  \n",
            "\n",
            "---  \n",
            "\n",
            "### Conclusão  \n",
            "\n",
            "LLMs de baixa latência não são apenas uma questão de “ser rápido”; são um diferencial competitivo que melhora a experiência do cliente, reduz custos operacionais e habilita casos de uso críticos em tempo real. Investir em otimizações de modelo, hardware adequado e arquitetura de deployment garante que sua empresa tire o máximo proveito da inteligência artificial, mantendo-se à frente no mercado.\n"
          ]
        }
      ]
    }
  ]
}